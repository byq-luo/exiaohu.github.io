---
layout: mypost
title: 图神经网络及其在交通预测方面的应用
categories: [Traffic prediction, Graph Neural Networks]
---

# 图神经网络

图神经网络可以粗略地分为基于谱的图神经网络和基于空间的图神经网络。其中前者从图的傅立叶变换而来，具有坚实的理论基础，在通过多番简化之后，其计算开销、局部性等已经能够满足大部分情形下的要求，但是它只能用来处理边上权重维数为$1$的无向图。而后者虽然缺乏理论基础，但是可以用来处理更加广泛的图结构数据，如有向图、多权图等。

## 基于谱的图神经网络

### 图的傅立叶变换和拉普拉斯矩阵

首先，连续函数的傅立叶变换可以表示为：
$$
\hat{f}(\omega) = \mathcal{F}[f(t)]=\int_{-\infty}^{\infty}f(t)e^{-i \omega t}dt
$$
其中$e^{-i \omega t}$是拉普拉斯算子$\nabla^2$的特征方程，即$\nabla^2 e^{-i \omega t} = \lambda e^{-i \omega t}$，通过它，函数$f(t)$就被映射到了以$\{e^{-i \omega t}\}$为基向量的空间中。与此类似，图的傅立叶变换定义为将其映射到以图的拉普拉斯矩阵的特征矩阵为基向量的空间中。

而图的拉普拉斯矩阵是什么呢？连续函数的拉普拉斯算子$\nabla^2 = \nabla \cdot \nabla$的第二个$\nabla$代表梯度，第一个$\nabla$代表散度，即连续的拉普拉斯算子指的是其梯度的散度。与此类似，图的拉普拉斯矩阵是图上的信号的梯度的散度。那么问题就变成了如何定义图上的梯度和散度。

我们知道，梯度是一个向量，其中的每一个分量都代表函数值在定义域的一个正交方向上下降的速度。在图中，每一个结点$i$都可以通过一个函数$f(\cdot)$映射到实域中的一个实数值$f(i) \in \mathbb{R}$，这样每一个结点都关联着一个实数值，这样假设图中有$N$个结点，$f(\cdot )$可以简写为一个向量$f \in \mathbb{R}^N$，这样的向量就称为图上的信号。

我们知道，函数的定义域可以分为若干个正交的方向，一个函数的定义域能够正交地分为几个方向是固定的，每个方向上都有一个梯度的分量，而梯度在这个方向上的分量$\nabla f_i = \frac{\partial f}{\partial x_i} = \frac{f(x + \triangle x) - f(x)}{\triangle x_i}$，即在这个方向上函数值的差值与距离的比值。而图上的每一个结点都连接有数目不等的边，这些边都可以看作是互相正交的方向，这样，与此类似，图上的信号在某一个方向（某一条边）上的梯度分量就是在这条边上图信号的差值与距离的比值。

那么问题就变成了如何定义图上两个结点在某一条边上的距离。我们知道，图上边上的权重代表这两个结点的接近程度（越近越大），而距离的定义与此相反，其表示两个结点离得有多远（越远越大），这样，距离可以定义为边上权重的倒数，这样，图上的两个结点$i$和$j$在边$e_{ij}$上的梯度就可以定义为$\nabla f_{ij}=(f_i - f_j) A_{ij}$，其中$A_{ij}$是边$e_{ij}$上的权重。

如百度百科所说，”散度是描述空气从周围汇合到某一处或从某一处流散开来程度的量“。函数的散度即其梯度在各个方向上的分量的和，$\nabla^2f=\sum_i{\nabla_i{f}}$。与此类似，图在结点$i$上的散度定义为该图信号在与该结点相连的每一条边上的梯度之和，即
$$
\nabla^2 f_i = \sum_j{(f_i - f_j)A_{ij}} = 
\left[
\begin{matrix}
f_i - f_1 & f_i - f_2 & \ldots & f_i - f_N
\end{matrix}
\right]
\left[
\begin{matrix}
A_{i,1} \\ A_{i,2} \\ \vdots \\ A_{i,N}
\end{matrix}
\right] = (f_i^T - f^T) \cdot A_i = f_i A_i - A_i^T \cdot f,
$$
其中$\cdot$是向量的点积。将$f_i A_i$简记为$f_i d_i$，其中$d_i=\sum_{j=1}^N{A_{ij}}$为结点$i$的度，即与其相连的所有边上的权重之和，上式可以简写为$\nabla^2 f_i = f_i d_i - A_i^T \cdot f$，因此，图在每个结点上的散度即
$$
\nabla^2 f = 
\left[
\begin{matrix}
f_1 d_1 - f^T \cdot A_1 \\ f_2 d_2 - f^T \cdot A_2 \\ \vdots \\ f_N d_N - f^T \cdot A_N
\end{matrix}
\right] =
\left[
\begin{matrix}
d_1 & 0 & \ldots & 0 \\ 0 & d_2 & \ldots & 0 \\ \vdots & \vdots & \ddots & \vdots \\ 0 & 0 & \ldots & d_N
\end{matrix}
\right]
\left[
\begin{matrix}
f_1 \\ f_2 \\ \vdots \\ f_N
\end{matrix}
\right] - 
\left[
\begin{matrix}
A_1^T \\ A_2^T \\ \vdots \\ A_N^T
\end{matrix}
\right]
\left[
\begin{matrix}
f_1 \\ f_2 \\ \vdots \\ f_N
\end{matrix}
\right] = (D - A)f,
$$
其中，$D$是对角线元素为图中每个结点的度的对角矩阵，$A$是该图的邻接矩阵，$A_{ij}$是结点$i$到结点$j$的权重。这样，图的散度$\nabla^2 f=(D-A)f$，即图的拉普拉斯矩阵$L$就是$\nabla^2=D-A$ 。

如前文所述，图的傅立叶变换定义为其与图的拉普拉斯矩阵的特征矩阵的卷积。那么，图的拉普拉斯矩阵经过特征分解$L=D-A=U \Lambda U^T$之后，在结点$i$上的图的傅立叶变换就是$\hat{f}_i=\sum_{j=1}^N{f_j U_j^T}$，那么在所有结点上同时做傅立叶变换，即
$$
\hat{f} = \mathcal{F}[f] = U^T f.
$$
傅立叶逆变换就是$f=U\hat{f}$。

最后，众所周知的是，在频域上的卷积可以转换为在谱域中的点积，然后转换回频域来进行运算。那么，图的卷积可以表示为
$$
f \star_{\mathcal{G}} g = \mathcal{F}^{-1}[\mathcal{F}[f] \cdot \mathcal{F}[g]] = U((U^T f) \cdot (U^T g)) = U ((U^T g) \cdot (U^T f) ),
$$
其中，$\star_{\mathcal{G}}$表示图卷积；$\cdot$表示向量点积，满足交换率；$g$是卷积核，可以通过学习得到。将$U^T g$看作可以学习的卷积核$g_{\theta}$，图卷积就是
$$
f \star_{\mathcal{G}} g_{\theta} = U(g_{\theta} \cdot (U^T f))=Ug(\theta)U^T f
$$
其中
$$
g({\theta})=
\left[
\begin{matrix}
\theta_1 & \ldots & 0 \\
\vdots & \ddots & \vdots \\
0 & \ldots & \theta_N \\
\end{matrix}
\right].
$$
接下来要介绍的图上频域卷积的工作，都是在此基础上做文章。

> Deep Convolutional Networks on Graph-Structured Data
>
> The emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular domains

### 切比雪夫多项式展开



> Convolutional Neural Networks on Graphs with Fast Localized Spectral Filtering

### 一阶切比雪夫多项式展开和GCN

Semi-Supervised Classification with Graph Convolutional Networks

## 基于空间的图神经网络

### 图卷积神经网络
Learning Convolutional Neural Networks for Graphs

### 图注意力神经网络 GAN
GRAPH ATTENTION NETWORKS

### 动态边权重的图神经网络
Dynamic Edge-Conditioned Filters in Convolutional Neural Networks on Graphs

### 扩散卷积神经网络 DCNN
Diffusion-Convolutional Neural Networks

### 基于消息传递的图神经网络框架
Graph Neural Networks: A Review of Methods and Applications

A Comprehensive Survey on Graph Neural Networks

### 超图神经网络和动态超图神经网络
Hypergraph Neural Networks

Dynamic Hypergraph Neural Networks

### 层次的图卷积和图池化
Hierarchical Graph Representation Learning with Differentiable Pooling

An end-to-end deep learning architecture for graph classification

Hierarchical Graph Convolutional Networks for Semi-supervised Node Classification

Graph U-Nets

Weighted Graph Cuts Without Eigenvectors: A Multilevel Approach. (Graclus算法)

## 图表示学习和网络嵌入
DeepWalk

LINE

Node2Vector

SDNE

DNGR

Inductive Representation Learning on Large Graphs (GraphSAGE)

HARP: Hierarchical Representation Learning for Networks

# 交通时空数据预测

## 基于循环神经网络的方法

### ___DCRNN___ (ICLR-2018)

DIFFUSION CONVOLUTIONAL RECURRENT NEURAL NETWORK: DATA-DRIVEN TRAFFIC FORECASTING

- 问题：交通预测
- 数据：`METR-LA`和`PEMS-BAY`
- 方法：使用**扩散卷积**代替GRU中的矩阵乘法，采用**Seq2Seq**结构，**课程学习**方法。

### MRes-RGNN (AAAI-2019)
Gated Residual Recurrent Graph Neural Networks for Traffic Prediction

- 问题：交通预测
- 数据：`METR-LA`和`PEMS-BAY`，但不使用标准的划分方式，而是同时利用长期的历史数据和外部特征。
- 方法：使用多个带门限残差连接的GCGRU(proposed by DCRNN)循环神经网络来捕捉不同的周期、短期时间依赖，然后加权求和得到最终的预测结果。具体的，使用两个分支，其中一个与DCRNN一样，用来捕捉邻近的时间关系，另一个用来捕捉Daily的周期关系，因为数据集小，所以无法使用Weekly的关系。外部信息首先经过embedding，然后和图信号concat在一起，再输入GCGRU。

### ST-MGCN (AAAI-2019)
Spatiotemporal Multi-Graph Convolution Network for Ride-hailing Demand Forecasting

- 问题：需求预测
- 数据：北京和上海的Ride-hailing订单，POI数据和OpenStreetMap提供的路网数据。
- 方法：分别计算距离相近、功能相似和交通连接的**多图**。首先使用上下文相关的门限神经网络增强RNN，基于外部环境数据计算权重来对不同时间戳的观测数据重新加权。在使用多图卷积。在所有结点的concatenated的原始特征和隐特征上做池化操作，将`N×T×P`数据转成`1×T×P`，然后再经过全连接，转成`T`维Attention权重，进行池化操作。在Attention操作之后，再将重新赋权后的特征放入RNN。

### ODMP和GEML (KDD-2019)
Origin-Destination Matrix Prediction via Graph Convolution: a New Perspective of Passenger Demand Modeling

- 问题：OD矩阵预测
- 数据：UCAR（神州优车）在北京和滴滴出行在成都的公开数据，分别一个月。
- 方法：**多图卷积**（距离相近和语义相近两张图），**图Attention**网络，**周期性跳跃连接的LSTM**。学习到每个结点的隐特征后，使用加权的点积聚合每两个结点的隐特征得到边上的最终预测结果。在预测主任务的同时，为了解决数据稀疏的问题，加入预测需求（每个结点的入度和出度）的副任务。

### ==ST-MetaNet== (KDD-2019)
Urban Traffic Prediction from Spatio-Temporal Data  Using Deep Meta Learning

- 问题：交通预测（同时做了flow预测和speed预测的实验）
- 数据：流量预测使用北京出租的数据，先分`32×32`的格子，速度预测使用`METR-LA`数据集，使用`DCRNN`的标准划分方法。
- 方法：**元知识学习**，分别从结点、边的原始特征通过神经网络获得RNN和图注意力网络的权重，然后使用Seq2Seq结构得到预测结果。其中，一个元学习器，输入为结点和边上的原始特征，输出为图注意力网络的权重；另一个元学习器，输入为结点特征，输出为RNN的权重，这样相当与每个结点都有自己的RNN。每个结点聚合特征所使用的图注意力网络都不一样。

### ==MRA-BGCN== (AAAI-2020)
multi-range attentive bicomponent graph convolutional network for traffic forecasting

- 问题：交通预测
- 数据：`METR-LA`和`PEMS-BAY`，使用`DCRNN`中提供的标准划分方式。
- 方法：在原图的基础上，构建以原图的边为结点的**边图**，建模边和边之间的关系，其中分别建模边和边之间顺序相连和相互竞争两种关系，用两个图卷积分别学边和结点的新特征。另一方面在时间建模时使用**Attention**对不同层次的特征图进行加权。

### ==ST-UNet==
ST-UNet: A Spatio-Temporal U-Network for Graph-structured Time Series Modeling

- 问题：时间序列建模
- 数据：`METR-LA`，使用`DCRNN`中提供的标准划分方式，`PeMS-M/L`，使用它自己的划分方式。
- 方法：结合**图池化**和**图反池化**，使用**U型结构**，先将图的分辨率降低，热爱和使用GCGRU学习时间关系。图池化使用非机器学习的方法，先将图分为若干个不相交的子图，然后每个子图上进行池化。

## 不基于循环神经网络的方法

### STGCN (IJCAI-2018)
Spatio-Temporal Graph Convolutional Networks: A Deep Learning Framework for Traffic Forecasting

- 问题：交通预测
- 数据：北京`BJER4`和`PeMSD7`两个传感器监测数据，时间窗口大小为5分钟，使用历史12个时间窗数据分别预测未来3、6、9个时间窗数据。
- 方法：空间上使用**图卷积**，时间上使用**门限因果卷积**。多层、三明治结构。每个块状结构包含两个时间卷积夹一个空间卷积在每个卷积内部都有残差连接。多个块状结构堆叠。模型本身是单步预测，在测试时循环进行多步预测。

### ==DGCNN== (AAAI-2019)
Dynamic Spatial-Temporal Graph Convolutional Neural Networks for Traffic Forecasting

- 问题：交通预测
- 数据：纽约传感器数据和`PeMS`传感器数据。时间窗口大小为5分钟，使用历史12个时间窗数据分别预测未来3、6、9个时间窗数据。
- 方法：模型的整体框架和`STGCN`是一样的。在此基础上，使用一个神经网络**对图拉普拉斯网络进行估计**。
- 记长期依赖的拉普拉斯阵为`$L_s$`，输入特征为 `$X \in \mathbb{R}^{N \times T \times F}$`，估计拉普拉斯矩阵的方法：
  - 首先通过两个矩阵`$U_1 \in \mathbb{R}^{N \times r_1}$`和`$ U_2 \in \mathbb{R}^{T \times r_2}$`（`$r_1 < N$`，`$r_2 < T$`）将其转为`$\hat{X} \in \mathbb{R}^{r_1 \times r_2 \times F}$`，再用`$U_1^T$`和`$U_2^T$`将其转为原来的形状，此时信息会有一些损失，将重构记为`$X_s$`，再计算`$X_e = X-Xs$`。
  - 将`$X_s$`和`$X_e$`用zero-mean归一化，再计算`$X_s X_e^T$`、`$X_e X_s^T$`和`$X_e X_e^T$`，将这三个`$N \times N$`的矩阵摞起来，再用两个`2-D`的卷积层将特征通道降到`1`，得到`$B=X_s X_e^T + X_e X_s^T + X_e X_e^T + Z_e$`，再通过公式
    ```math
    L_e = \sum_{i=1}^I(-1)^i L_s (BL_s)^i + o(BL_s)
    ```
    在实验中，`$I$`设置为`6`，此时增加`$I$`也不会明显增加模型精度。
  - 最后得到了`$L = L_s + L_e$`，然后对其进行归一化。

### ASTGCN (AAAI-2019)
Attention Based Spatial-Temporal Graph Convolutional Networks for Traffic Flow Forecasting

- 问题：交通流预测
- 数据：`PeMSD4`和`PeMSD8`两个传感器监测数据，时间窗口大小为5分钟，使用历史24个每周相同时间、12个每天相同时间、24个邻近的历史时间窗，来预测未来12个时间窗。
- 方法：使用三个分支分别处理历史每周的、每天的和邻近的数据，使用堆叠的`ST Block`处理，然后跟一个全连接层，最后对三个结果加权求和。每个`ST Block`分为空间和时间的注意力机制`SAtt+TAtt`和空间上的图卷积和时间上的标准卷积`GCN+Conv`。

### STG2Seq (IJCAI-2019)
STG2Seq: Spatial-temporal Graph to Sequence Model for Multi-step Passenger Demand Forecasting

- 问题：多步乘客需求预测
- 数据：沈阳的滴滴数据（时间窗为1小时），纽约的共享单车数据（时间窗为1小时），北京的出租车数据（时间窗为0.5小时）
- 方法：模型分为三部分：`长期时间依赖编码器`、`短期时间依赖编码器`和基于注意力机制的`输出模块`：
  - 建图：计算每两个区域之前看客流的相似度（即历史需求量的皮尔逊相关系数），当相似读大于某个阈值时，邻接矩阵设为1,否则设为0；
  - `长期时间依赖编码器接收`最近的`$h$`个时间窗口的历史数据，
  - `短期时间依赖编码器`接收已经预测出结果的`$q$`个时间窗口的数据，即在预测第`$T \in [t+1, t+\tau]$`个时间窗口的需求量时，输入数据为`$T-q$`到`$T-1$`的历史数据或预测结果；
  - `GGCM`是`长期时间依赖编码器`和`短期时间依赖编码器`的基础组建，这俩部分除了输入特征的时间窗口不同之外，都是由堆叠的`GGCM`组成的。而一个`GGCM`由两个`GCN`组成，每个`GCN`同时应用在时间和空间维度上（记时间维度的卷积核大小为`$k$`，在时间维度上做单侧长度为`$k-1$`的padding以模拟因果卷积，然后将`$k$`个时间窗的特征维度合并在一起再输入GCN），这样避免了显式的进行时间维度的卷积。一个`GCN`用来计算值，另一个`GCN`用来计算门限，在每两个`GGCM`之间使用残差和门限机制
  - 基于注意力机制的`输出模块`分别在时间维度和特征维度应用Attention，得到最终的单步预测结果。

### ==Graph WaveNet== (IJCAI-2019)
Graph WaveNet for Deep Spatial-Temporal Graph Modeling

- 问题：时空图建模
- 数据：`METR-LA`和`PEMS-BAY`，使用`DCRNN`中提供的标准划分方式。
- 方法：
  - 自适应的邻接矩阵：通过两个可训练的结点嵌入`$E_1,E_2 \in R^{N \times c}$`，自适应的扩散矩阵`$\tilde{A}=softmax(ReLU(E_1E_2^T)).$`。在预定义的邻接矩阵存在时，可以将学到的和预定义的共同使用，而当其不存在时，可以单独使用自适应的邻接矩阵。
  - 结合门限的空洞的因果卷积和图卷积同时学习时空依赖。
  - 一次性输出所有多步结果，而不使用迭代的方式逐步输出

### GSTNet (IJCAI-2019)
GSTNet: Global Spatial-Temporal Network for Traffic Flow Prediction

- 问题：交通流预测（单步预测）
- 数据：`北京地铁数据`（时间窗10分钟）、`北京公交数据`（时间窗1小时）和`北京出租车GPS数据`（时间窗20分钟）
- 方法：堆叠的时空块，每个时空块包括一个`多分辨率的时间模块`和一个`全局相关的空间模块`：
  - `多分辨率的时间模块`：使用多层的因果卷积（步长和卷积核的大小相等，利用`0`填充让时间长度不变），然后将每层的特征图堆叠在一起产生多分辨率的输出，再用一个卷积核大小为1维的卷积将通道数降低
  - `全局相关的空间模块`：一个局部化的图卷积模块和一个非局部的相关机制（相当于一个全连接的图卷积，其邻接矩阵根据特征相关性计算得到）。加入残差连接。
  - `输出层`：时间维度的注意力机制，